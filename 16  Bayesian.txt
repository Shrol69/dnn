# Bayesian

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical

# Load MNIST data
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = x_train.reshape(-1, 28*28) / 255.0
x_test = x_test.reshape(-1, 28*28) / 255.0
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# Bayesian Neural Network parameters
input_size = 784
hidden_size = 64
output_size = 10
learning_rate = 0.01
epochs = 20
print("BNN Architechture")
print("input_size = 784 \nhidden_size = 64 \noutput_size = 10 \nlearning_rate = 0.01")

# Initialize means and stddevs for weights and biases
np.random.seed(42)
W1_mu = np.random.randn(input_size, hidden_size) * 0.1
W1_sigma = np.ones((input_size, hidden_size)) * 0.1
b1_mu = np.zeros(hidden_size)
b1_sigma = np.ones(hidden_size) * 0.1

W2_mu = np.random.randn(hidden_size, output_size) * 0.1
W2_sigma = np.ones((hidden_size, output_size)) * 0.1
b2_mu = np.zeros(output_size)
b2_sigma = np.ones(output_size) * 0.1

def relu(x):
    return np.maximum(0, x)

def softmax(x):
    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))
    return e_x / np.sum(e_x, axis=1, keepdims=True)

def cross_entropy(predictions, targets):
    return -np.mean(np.sum(targets * np.log(predictions + 1e-9), axis=1))

def accuracy(predictions, targets):
    return np.mean(np.argmax(predictions, axis=1) == np.argmax(targets, axis=1))

# For learning curve tracking
loss_curve = []
accuracy_curve = []

for epoch in range(epochs):
    # Sample weights and biases from distributions
    W1 = W1_mu + W1_sigma * np.random.randn(*W1_mu.shape)
    b1 = b1_mu + b1_sigma * np.random.randn(*b1_mu.shape)
    W2 = W2_mu + W2_sigma * np.random.randn(*W2_mu.shape)
    b2 = b2_mu + b2_sigma * np.random.randn(*b2_mu.shape)

    # Forward pass
    z1 = x_train.dot(W1) + b1
    a1 = relu(z1)
    z2 = a1.dot(W2) + b2
    y_pred = softmax(z2)

    # Compute loss and accuracy
    loss = cross_entropy(y_pred, y_train)
    acc = accuracy(y_pred, y_train)
    loss_curve.append(loss)
    accuracy_curve.append(acc)

    # Print epoch info
    print(f"Epoch {epoch + 1}")
    print("Weights and Biases before training step:")
    print("W1 mean:", np.mean(W1), "W2 mean:", np.mean(W2))
    print("b1 mean:", np.mean(b1), "b2 mean:", np.mean(b2))
    print(f"Training Accuracy: {acc:.4f}, Loss: {loss:.4f}")

    # Backpropagation (gradient approximation)
    dL_dz2 = y_pred - y_train
    dL_dW2 = a1.T.dot(dL_dz2) / x_train.shape[0]
    dL_db2 = np.mean(dL_dz2, axis=0)

    dL_da1 = dL_dz2.dot(W2.T)
    dL_dz1 = dL_da1 * (z1 > 0)
    dL_dW1 = x_train.T.dot(dL_dz1) / x_train.shape[0]
    dL_db1 = np.mean(dL_dz1, axis=0)

    # Update means (gradient descent on mu only)
    W1_mu -= learning_rate * dL_dW1
    b1_mu -= learning_rate * dL_db1
    W2_mu -= learning_rate * dL_dW2
    b2_mu -= learning_rate * dL_db2

    print("Parameter changes (mean deltas):")
    print("W1_mu change:", np.mean(-learning_rate * dL_dW1))
    print("W2_mu change:", np.mean(-learning_rate * dL_dW2))
    print("b1_mu change:", np.mean(-learning_rate * dL_db1))
    print("b2_mu change:", np.mean(-learning_rate * dL_db2))
    print("-" * 50)

# Plot learning curves
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(range(1, epochs + 1), loss_curve, marker='o', label='Training Loss')
plt.title("Loss Curve")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(range(1, epochs + 1), accuracy_curve, marker='o', label='Training Accuracy')
plt.title("Accuracy Curve")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()

plt.tight_layout()
plt.show()
